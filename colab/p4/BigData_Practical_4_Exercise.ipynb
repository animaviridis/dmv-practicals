{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigData - Practical 4 - Exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3vxT1AGOzZZ",
        "colab_type": "text"
      },
      "source": [
        "Data Mining and Visualisation 2019-2020 <br>\n",
        "Practical 4 - Big Data Mining <br>\n",
        "Teaching Assistant Muhammad Usman <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_1Bo6TrO8g3",
        "colab_type": "text"
      },
      "source": [
        "This practical requires you to use apache spark and spark ml in a standalone mode to run a machine learning pipeline using linear regression. A dataset has been provided for this that enables you to test and experiment yourself with a good chunk of the Apache Ecosystem.\n",
        "Some parts of the code are provided, some others need to be completed by yourselves. In short you are going to use linear regression to predict game sales <br>\n",
        "\n",
        "Use this notebook with Google CoLab.\n",
        "\n",
        "Although not needed, you might enable the GPU capability; select Edit, Notebook setting and from the drop down menu select gpu. The notebook will then reconnect and assign a GPU that you can use for free.\n",
        "\n",
        "If you run the command included below in the main code, you can see the GPU allocation you have been provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tnvGgNpOrcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiYITyPPT2mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovDTWVnnT5R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voreTI2BT7Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncs-gkJMT9Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To find the GPU information allocated to play with run the below snippet of code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnBinmezVGw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CG2l8cHVJfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6m_KUtZVMAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq5xLbAnVOD7",
        "colab_type": "text"
      },
      "source": [
        "Up until that point what you do is simply importing and installing everything that's needed to have a spark cluster created. The files.upload() command will enable you to upload the dataset provided, i.e. vgsales.csv, so that you can then start the data mining bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdBY5FR3Vfgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read the file using spark read command\n",
        "\n",
        "# you may want to count the data and show them to understand what is all about\n",
        "\n",
        "# print out the schema\n",
        "\n",
        "# show the game name for each platform; choose a number of rows you would like to show\n",
        "\n",
        "# you could run some descriptive statistics for the NA_sales and EU_sales"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQmgmbU-dOQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now group the data by platform and order them by count; show the first 10 rows in descending order\n",
        "\n",
        "# now use pyspark sql types to change the column type to double for \"Year\", \"NA_Sales\", \"EU_Sales\", \"JP_Sales\"\n",
        "\n",
        "# e.g. data2 = data.withColumn(\"Year\", data[\"Year\"].cast(DoubleType()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaJKl0ktjsdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "inputcols = [\"Global_Sales\", \"NA_Sales\", \"EU_Sales\"]\n",
        "assembler = VectorAssembler(inputCols= inputcols,\n",
        "                            outputCol = \"predictors\")\n",
        "predictors = assembler.transform(data)\n",
        "predictors.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYzx87pRjuOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_data = predictors.select(\"predictors\", \"JP_Sales\")\n",
        "model_data.show(5,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAQK0kMojwmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train_data and test_data randomly at 80%/20%\n",
        "# from pyspark.ml.regression import LinearRegression\n",
        "# create a model using the predictors created above using as labels the JP_Sales\n",
        "#fit and evaliate the model on test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkYAO8BgkHRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrModel.coefficients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4CFlIKLkJCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred.predictions.show(20)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDMKxC1ukK6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use mse, rmse, mae and r^2 as metrics.... you may find them at from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DS8QtrQkbM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse = eval.evaluate(pred.predictions)\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXazK601kb1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0-Ycgwbkdbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yni5tmtkePM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaB23txIke9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}